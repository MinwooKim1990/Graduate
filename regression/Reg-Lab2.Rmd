---
title: "ASML Regression, Computer Lab 2"
author: "Jochen Einbeck"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Real estate prize data: Preliminaries

Consider again the data set on real estate prizes extracted from [kaggle](https://www.kaggle.com/quantbruce/real-estate-price-prediction/).

You can read in the data via

```{r}
realestate<- read.csv("http://www.maths.dur.ac.uk/~dma0je/Data/Realestate.csv", header=TRUE)
```

We do the same simplifications of long variable names as in the last practical:

```{r}
colnames(realestate)[8]<- "Y"
colnames(realestate)[5]<- "X4.stores"
colnames(realestate)[4] <- "X3.MRT"
```

## Multiple linear regression

Clearly, the simple linear regression model considered in the previous lab is not yet a very good model, so let us include more variables. Therefore, fit a multiple linear regression model, with all variables $x_1, \ldots, x_6$ as predictors, and store it into an object `real.fit2`.

*Note*: You can add multiple predictors to `lm' by seperating them through`+`symbols after the`\~\`.

```{r}
# real.fit2  <- lm(Y~ X1.transaction.date +X2.house.age+..., data=realestate)

```

Display the `summary` of the fitted object. Note that the displayed table may be hard to read; something like `round(summary(real.fit2)$coef, digits=4)` will look better.

```{r}
# ...
```

Adapt the previously introduced bootstrap methodology to this scenario.

```{r}
# ...
```

Produce a matrix which in the first row contains the standard errors of the linear model fitted through the `lm` function, and in the second row the standard errors obtained through the Bootstrap procedure.

```{r}
# ...
```

Let's now apply again Jags. One thing that you need to know here is that, for multiple linear regression, the MCMC routine will be very inefficient (small acceptance rates) if the predictors are not scaled (to zero mean and unit variance). Therefore, we need to scale them. This could be done for instance as follows:

```{r}
# The design matrix without the intercept
realX <- as.matrix(realestate[,2:7])

# We need to keep the means and sd.'s for later `unscaling':
m <- apply(realX, 2,mean)
S <- apply(realX, 2, sd)

# This is the actual scaling step:
SrealX<-scale(realX)

# Check:
apply(SrealX, 2, mean)
apply(SrealX,2,sd)

```

Implement from here on the machinery as usual, but with the obvious adjustments as we have now multiple predictors.

```{r}
model2_string <- "model{
 # ...
}"

```

Finally, execute the actual MCMC sampling and the collection of posterior samples:

```{r}
#  model2 <- ...

#  postmod2.samples = coda.samples(model2, c("beta0", "beta", "sigma"), 10000)[[1]]
#  summary(postmod2.samples)

```

If you look at the `summary` output of the posterior samples you see that the estimates do not quite seem to make sense, except for `sigma` and `beta0`. This is due to the scaling step executed previously. To obtain sensible results, we need to un-scale them. This can be done for instance via

```{r}
# rowMeans(t(postmod2.samples[,1:6])/S)
# round(apply(t(postmod2.samples[,1:6])/S, 1,sd), digits=4)

```

(Mathematical proof of why this is the right way of un-scaling will be provided in the solution file).

This should lead to similar values as obtained for the LM and bootstrap.

## Smoothing - Lidar data

We consider a data set with 221 observations from a a light detection and ranging (LIDAR) experiment. Please use the following code to read in, and display the data. Also read the help file.

```{r}
library(SemiPar)
data(lidar)
plot(lidar)
# help(lidar)
```

It is sometimes useful to "attach" an object to our workspace, so that we can directly access its components. Here this is the case, so let's do this:

```{r}
# attach(lidar)
```

Identify a suitable bandwidth "by eye", and fit a local smoother to the data using `locpoly`:

```{r}
# require(KernSmooth)
# ...

```

Adjust your bandwidth used above until the curve fits adequately through the data:

```{r}
# ...

```

Now, consider the use of smoothing splines, using function `sm.spline`, with its default options (Slide 6).

```{r}
# require(pspline)
# ...

```

Display the fitted smooth curve.

```{r}
# ...

```

Inspect the fitted object `lidar.spline` (by simply `print`ing it).

```{r}
# ...

```

This gives a few useful pieces of information (that we did not get for `locpoly`). Specifically,

-   "Smoothing Parameter": This is the analogue to our bandwidth (with this smoothing technique, it is a penalty parameter, $\lambda$).
-   Equivalent degree of freedom, is the number of parameters that one would need to fit a parametric model of similar flexibility. This corresponds to the trace of the smoother matrix (see Slide 19).
-   CV Criterion. Value of $CV(\lambda)$ after completion of cross-validation (see Slide 19).
-   GCV Criterion. Value of $GCV(\lambda)$ after completion of generalized cross-validation. This is a quicker variant of cross-validation where $\ell_{ii}$ in the denominator is replaced by $\frac{1}{n}\sum \ell_{ii}$.

*Task*: Find out whether CV or GCV has been used. In either case, re-fit the curve using the other, and compare results.

```{r}
# ...

```

Finally, produce a \`family plot' over a suitable range of smoothing parameters (Slide 24), using either kernels or smoothing splines.

```{r}
# lid.fam.fit <- matrix(0,25,401)
# ...

```
