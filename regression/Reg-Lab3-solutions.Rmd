---
title: "ASML Regression, Computer Lab 3"
author: "Jochen Einbeck"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


# Preliminaries

Consider again the data set on real estate prizes extracted from  [kaggle](https://www.kaggle.com/quantbruce/real-estate-price-prediction/).

You can read in the data via

```{r}
realestate<- read.csv("http://www.maths.dur.ac.uk/~dma0je/Data/Realestate.csv", header=TRUE)
```

We do the same simplifications of long variable names as in the last practicals:

```{r}
colnames(realestate)[8]<- "Y"
colnames(realestate)[5]<- "X4.stores"
colnames(realestate)[4] <- "X3.MRT"
```

##  Multiple regression using ridge and lasso penalties

Use the following code to load R package `glmnet` and to produce a $414 \times 6$ data matrix which only contains the predictor variables:

```{r}
require(glmnet)
realX<-  as.matrix(realestate[,2:7])
dim(realX)
```

Carry out ridge regression using `glmnet` as in the lecture, and apply `plot` on the fitted object with option `xvar="lambda"` to see the trajectories of estimated parameters.

```{r}
real.ridge<- glmnet(realX, realestate$Y, family="gaussian", alpha=0)
real.ridge
plot(real.ridge, xvar="lambda")

```


We next carry out cross-validation in order to select the penalty parameter $\lambda$.  You can  use the following code sequence in order to do this. Make it clear to yourself what this code actually does.

```{r}
real.ridge.cv=  cv.glmnet(realX, realestate$Y, alpha=0 )
par(mfrow=c(1,1), mai=c(0.8,0.8,0.8,0.8), cex.axis=2)
plot(real.ridge.cv)
real.ridge.cv$lambda.min
real.ridge.coef<- coef(real.ridge.cv, s="lambda.min")[,1]
```

Now reproduce all steps carried out for ridge regression, this time using the lasso (that is, `alpha=1`).

```{r}
real.lasso<- glmnet(realX, realestate$Y, family="gaussian", alpha=1)
real.lasso
plot(real.lasso, xvar="lambda")
real.lasso.cv=  cv.glmnet(realX, realestate$Y, alpha=1 )
par(mfrow=c(1,1), mai=c(0.8,0.8,0.8,0.8), cex.axis=2)
plot(real.lasso.cv)
real.lasso.cv$lambda.min
real.lasso.coef<- coef(real.lasso.cv, s="lambda.min")[,1]

```

Create a $3 \times 6$ matrix which in the first row has the parameter estimates from ordinary least squares regression, in the second row those from ridge regression, and in the third row those from the lasso. Give each row appropriate names.



```{r}
all.estimates<- rbind(real.fit2$coef,
        real.ridge.coef,
       real.lasso.coef
      )
rownames(all.estimates)<-c("LS", "Ridge", "Lasso")
all.estimates
```


Adapt the previously introduced bootstrap methodology to the ridge regression scenario, and save the standard errors of the model parameters into an object `real.ridge.boot.sd`


```{r}
B <- 199 # ideally 999 or so, but used 199 here for quick compilation
n <- dim(realX)[1]
Ynew <- matrix(0, n, B)
X <- model.matrix(real.fit2)
Y<- realestate$Y
Xnew<-list()
p<- dim(X)[2]
for (b in 1:B){
  j<-sample(n, replace=TRUE)
  Xnew[[b]]<- X[j,2:7]
  Ynew[,b]<-Y[j]
}

real.ridge.boot<-matrix(0, B, p)

for (b in 1:B){
  real.cv<- cv.glmnet(Xnew[[b]], Ynew[,b], alpha=0 )
  real.bridge<-  coef(real.cv , s="lambda.min" )
  real.ridge.boot[b,]<- as.vector(real.bridge)
  if (b%%10==0){print(b)}
}

boxplot(real.ridge.boot[,2:7])

real.ridge.boot.sd<- apply(real.ridge.boot, 2, sd)
real.ridge.boot.sd

#abline(0,0,col=3)

```

Create a new $7 \times 2$ matrix `real.ridge.fit` which in the first row contains the ridge estimates and in the second row their bootstrap standard errors. Then devide the first row by the second, yielding "bootstrapped ridge regression t values". Visualize these through a `barplot' and draw horizontal lines at $\pm 2$. Interpret.

```{r}
real.ridge.fit<- rbind(real.ridge.coef, real.ridge.boot.sd)
real.ridge.t<- real.ridge.fit[1,]/real.ridge.fit[2,]
barplot(real.ridge.t)
abline(2,0, col=2)
abline(-1,0, col=2)
```






# Bayesian ridge regression

Use now Jags to implement a Bayesian version of ridge regression. It is advised to use again scaled predictors for this purpose.

*Note*: The only actual difference to the Jags implementation for usual multiple linear regression is that the prior precision of the `beta[j]` is now `tau*lambda`, for some fixed ridge parameter `lambda` for which you can assign an arbitrary value, such as 25, to start with.  Of course,at this stage, we do not know which values of $\lambda$ are good or bad, so any other value (rather than 25) could be taken here.  The value 25 was a one-shot guess resulting in estimates that are close to those from the frequentist ridge estimator.

```{r}
# Means and sd.'s
m <- apply(realX, 2,mean)
S <- apply(realX, 2, sd)

# This is the actual scaling step:
SrealX<-scale(realX)

# Check:
apply(SrealX, 2, mean)
apply(SrealX,2,sd)

```


```{r}
model4_string <- "model{
  for(i in 1:N){
    y[i] ~ dnorm(mu[i], tau)   # tau = precision=inverse variance
    mu[i] <- beta0+beta%*%X[i,]
  }
  # Prior distribution on mean
    beta0 ~ dnorm(0, 0.01);
    for (j in 1:6){
      beta[j]~  dnorm(0, tau*lambda)
    }
    tau  ~ dgamma(0.01, 0.01)
    sigma<- 1/sqrt(tau)
    lambda=25
   
}"

```

Execute the actual MCMC sampling and the collection of posterior samples as usual:

```{r}
require(rjags)
model4 <- jags.model(textConnection(model4_string),
          data = list(X=SrealX, y=realestate$Y,N=dim(realX)[1]))

update(model4, 10000)

postmod4.samples = coda.samples(model4, c("beta0", "beta", "tau"), 10000)[[1]]

summary(postmod4.samples)

```

These need again be un-scaled. Compare the posterior mean estimates directly with the frequentist ridge estimates.

```{r}

rbind(
real.ridge.coef[2:7],
rowMeans(t(postmod4.samples[,1:6])/S)
)
```

Now, modify your code to give a suitable prior to $\lambda$, and estimate it alongside the other model parameters in Jags.

```{r}
model5_string <- "model{
  for(i in 1:N){
    y[i] ~ dnorm(mu[i], tau)   # tau = precision=inverse variance
    mu[i] <- beta0+beta%*%X[i,]
  }
  # Prior distribution on mean
    beta0 ~ dnorm(0, 0.01);
    for (j in 1:6){
      beta[j]~  dnorm(0, tau*lambda)
    }
    tau  ~ dgamma(0.01, 0.01)
    sigma<- 1/sqrt(tau)
    lambda ~ dgamma(5,1/5)
   
}"

```

Here we are using a prior distribution for $\lambda$. Note that the expectation of a Gamma distribution with parameters (5,1/5) is just 25; so our choice of prior is informed by our fixed choice of $\lambda$ above. But, we are allowing for a large variance of 5/(1/25)=125.
Of course, other priors may be used, and it is an interesting exercise to observe the sensitivity of the results to this choice.


```{r}
require(rjags)
model5 <- jags.model(textConnection(model5_string),
          data = list(X=SrealX, y=realestate$Y,N=dim(realX)[1]))

update(model5, 10000)

postmod5.samples = coda.samples(model5, c("beta0", "beta", "tau", "lambda"), 10000)[[1]]

summary(postmod5.samples)

```

Visualize the sampled posterior distributions using `plot`:

```{r}
# plot(postmod5.samples)
```

Compare the posterior mean estimates again to previously fitted models, and summarize conclusions.

```{r}
rbind(
real.ridge.coef[2:7],
rowMeans(t(postmod5.samples[,1:6])/S)
)
```

Again, we see a similar effect for the frequentist and the Bayesian ridge regression, with the latter now providing a posterior median estimate for lambda at about 13. It appears that parametrizations used in `glmnet` and `rjags` are not the same (that is, a value of magnitude $\lambda\approx 10$ in `rjags` corresponds to $\lambda \approx 1$ in `glmnet` );  the way that the intercept and the scaling are being dealt with in both approaches is probably playing  an additional role.







