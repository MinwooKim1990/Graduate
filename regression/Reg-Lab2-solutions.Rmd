---
title: "ASML Regression, Computer Lab 2"
author: "Jochen Einbeck"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

## Real estate prize data: Preliminaries

We consider again the data set on real estate prizes extracted from [kaggle](https://www.kaggle.com/quantbruce/real-estate-price-prediction/).

You can read in the data via

```{r}
realestate<- read.csv("http://www.maths.dur.ac.uk/~dma0je/Data/Realestate.csv", header=TRUE)
```

We do the same simplifications of long variable names as in the last practical:

```{r}
colnames(realestate)[8]<- "Y"
colnames(realestate)[5]<- "X4.stores"
colnames(realestate)[4] <- "X3.MRT"
```

## Multiple linear regression

Clearly, the simple linear regression model considered in the previous lab is not yet a very good model, so let us include more variables. Therefore, fit a multiple linear regression model, with all variables $x_1, \ldots, x_6$ as predictors, and store it into an object `real.fit2`.

*Note*: You can add multiple predictors to `lm` by seperating them through `+` symbols after the `~`.

```{r}
real.fit2<- lm(Y~ X1.transaction.date + X2.house.age + X3.MRT + X4.stores
               + X5.latitude + X6.longitude, data=realestate)

```

Display the `summary` of the fitted object. Note that the displayed table may be hard to read; something like `round(summary(real.fit2)$coef, digits=4)` will look better.

```{r}
summary(real.fit2)
round(summary(real.fit2)$coef, digits=4)
```

Adapt the previously introduced bootstrap methodology to this scenario.

```{r}
B <- 199
n <- dim(realestate)[1]
Ynew <- matrix(0, n, B)
X <- model.matrix(real.fit2)
Y<- realestate$Y
Xnew<-list()
p<- dim(X)[2]
for (b in 1:B){
  Xnew[[b]]<- X
  for (i in 1:n){
    j<-sample(n,1)
    Xnew[[b]][i,]<- X[j,]
    Ynew[i,b] <-Y[j]
  }
}

all.real.boot <-matrix(0, B, p)

for (b in 1:B){
  real.fitb<- lm(Ynew[,b]~Xnew[[b]])
  all.real.boot[b,]<- summary(real.fitb)$coef[,1]
}

all.boot2.sd<- apply(all.real.boot, 2, sd)
```

Produce a matrix which in the first row contains the standard errors of the linear model fitted through the `lm` function, and in the second row the standard errors obtained through the Bootstrap procedure.

```{r}
both.real.sd <- rbind( summary(real.fit2)$coef[,2], all.boot2.sd)
both.real.sd
```

Let's now apply again Jags. One thing that you need to know here is that, for multiple linear regression, the MCMC routine will be very inefficient (small acceptance rates) if the predictors are not scaled (to zero mean and unit variance). Therefore, we need to scale them. This could be done for instance as follows:

```{r}
# The design matrix without the intercept
realX <- as.matrix(realestate[,2:7])

# We need to keep the means and sd.'s for later `unscaling':
m <- apply(realX, 2,mean)
S <- apply(realX, 2, sd)

# This is the actual scaling step:
SrealX<-scale(realX)

# Check:
apply(SrealX, 2, mean)
apply(SrealX,2,sd)

```

Implement from here on the machinery as usual, but with the obvious adjustments as we have now multiple predictors.

```{r}
model2_string <- "model{
  for(i in 1:N){
    y[i] ~ dnorm(mu[i], tau)   # tau = precision=inverse variance
    mu[i] <- beta0+beta%*%X[i,]
  }
  # Prior distribution on mean
    beta0 ~ dnorm(0, 0.0001);
    for (j in 1:6){
      beta[j]~  dnorm(0, 0.0001)
    }
    tau  ~ dgamma(0.01, 0.01)
    sigma<- 1/sqrt(tau)
   
}"

```

Finally, execute the actual MCMC sampling and the collection of posterior samples:

```{r}
require(rjags)
model2 <- jags.model(textConnection(model2_string),
            data = list(X=SrealX, y=Y,N=dim(realX)[1])
          )

update(model2, 10000)

postmod2.samples = coda.samples(model2, c("beta0", "beta", "sigma"), 10000)[[1]]

summary(postmod2.samples)

```

If you look at the `summary` output of the posterior samples you see that the estimates do not quite seem to make sense, except for `sigma`. This is due to the scaling step executed previously. To obtain sensible results, we need to un-scale them. To see how to do this, denote the model of interest (non-scaled) by

$y=\alpha+ \sum_{j=1}^p \beta_j x_j$

where the intercept, $\alpha$, has been separated from the other model parameters, and the error term has been omitted for ease of presentation. Let $m_j$ denote the mean of all observations from the $j$th predictor variable. Then the scaled predictor variables can be written as (this is exactly what function `scale` does)

$x_j^*= (x_j-m_j)/s_j$

Let us write the model for the scaled predictors as

$y= \alpha^*+ \sum_{j=1}^p \beta_j^* x_j^*$

The question is: How to write $\alpha$, $\beta_j$ in terms of $\alpha^*$, $\beta_j^*$?

Answer: We see that

$y= \alpha^*+ \sum_j \frac{x_j-m_j}{s_j}\beta_j^* = \sum_j \frac{\beta_j^*}{s_j} x_j - \sum_j \frac{m_j}{s_j}\beta_j^*+\alpha^*$

so that clearly

$\beta_j= \beta_j^*/s_j$ and

$\alpha= \alpha^*- \sum_j \frac{m_j}{s_j}\beta_j^*= \alpha^*- \sum m_j \beta_j$

This gives rise to the following code (incl. comparison with frequentist linear model fit):

```{r}
# beta_j
beta.jags<- rowMeans(t(postmod2.samples[,1:6])/S)
rbind(beta.jags,
      real.fit2$coef[2:7])

# alpha
real.fit2$coef[1]
mean(postmod2.samples[,7])-m%*%beta.jags

```

We see good agreement with the frequentist linear model.

Similar, one can recover the standard errors for the beta parameters as follows:

```{r}
sebeta.jags <- round(apply(t(postmod2.samples[,1:6])/S, 1,sd), digits=4)
rbind(
 sebeta.jags,
 summary(real.fit2)$coef[2:7,2]
)
```

## Smoothing - Lidar data

We consider a data set with 221 observations from a a light detection and ranging (LIDAR) experiment. Please use the following code to read in, and display the data. Also read the help file.

```{r}
require(SemiPar)
data(lidar)
plot(lidar)
# help(lidar)
```

It is sometimes useful to "attach" an object to our workspace, so that we can directly access its components. Here this is the case, so let's do this:

```{r}
attach(lidar)
```

Identify a suitable bandwidth "by eye", and fit a local smoother to the data using locpoly:

```{r}
require(KernSmooth)
lidar.loc <- locpoly(range, logratio, bandwidth=50)
names(lidar.loc)
plot(range, logratio)
lines(lidar.loc, col=2)
```

Adjust your bandwidth used above until the curve fits adequately through the data:

```{r}
lidar.loc2 <- locpoly(range, logratio, bandwidth=20)
plot(range, logratio)
lines(lidar.loc2, col=2)
```

Now, consider the use of smoothing splines, using function `sm.spline`, with its default options (Slide 6).

```{r}
require(pspline)
lidar.spline <-  sm.spline(range,  logratio)
```

Display the fitted smooth curve.

```{r}
plot(range, logratio)
lines(lidar.spline$x, lidar.spline$ysmth, col=2, lwd=2)
```

Inspect the fitted object `lidar.spline` (by simply `print`ing it).

```{r}
print(lidar.spline)
```

This gives a few useful pieces of information (that we did not get for `locpoply`). Specifically,

-   "Smoothing Parameter": This is the analogue to our bandwidth (with this smoothing technique, it is a penalty parameter, $\lambda$).
-   Equivalent degree of freedom, is the number of parameters that one would need to fit a parametric model of similar flexibility. This corresponds to the trace of the smoother matrix (see Slide 19).
-   CV Criterion. Value of $CV(\lambda)$ after completion of cross-validation (see Slide 19).
-   GCV Criterion. Value of $GCV(\lambda)$ after completion of generalized cross-validation. This is a quicker variant of cross-validation where $\ell_{ii}$ in the denominator is replaced by $\frac{1}{n}\sum \ell_{ii}$.

*Task*: Find out whether CV or GCV has been used. In either case, re-fit the curve using the other, and compare results.

```{r}
# sm.spline(x, y, w, cv=FALSE, ...), so GCV had been used.
lidar.spline2 <-  sm.spline(range,  logratio, cv=TRUE)
print(lidar.spline2)
plot(range, logratio)
lines(lidar.spline$x, lidar.spline$ysmth, col=2, lwd=1)
lines(lidar.spline2$x, lidar.spline2$ysmth, col=3, lwd=1)
```

Finally, produce a \`family plot' over a suitable range of smoothing parameters (Slide 24), using either kernels or smoothing splines.

```{r}
lid.fam.fit<-matrix(0,50,401)
h<-seq(2,50, by=2)
for (j in 1:25){
  lid.fam.fit[j,]<-locpoly(range, logratio,
     bandwidth=h[j])$y
}

plot(range, logratio)
for (j in 1:25){
  lines(lidar.loc$x, lid.fam.fit[j,], col="grey50")
}
```
